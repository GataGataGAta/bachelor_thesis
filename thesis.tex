% Last updated at 2024/12/10 09:39:36
\documentclass[12pt,a4j]{ltjsreport}
\usepackage{ltklthesis}
\usepackage{multirow}
\usepackage{float}
\usepackage{amsmath}
\usepackage{graphbox}

\begin{document}

\type{B} %卒論はB，修論はM
\author{山形隼士}
\id{2503} %学籍番号
\fyear{2026} %修了・卒業年(修了・卒業式が2025年3月なら2025)
\title{審判ジェスチャに着目した\\バスケットボールシュート種類\\判別手法}
\abstract{
	近年，スポーツアナリティクスの発展に伴い，バスケットボール競技におけるスタッツの重要性が高まっており，スタッツを収集することを自動化することが求められている．特に映像解析を用いたシュート種類の自動判別においては，選手位置をコート平面へ投影する手法が一般的だが，オクルージョンやホモグラフィ変換の誤差により正しく判別できないという課題があった．
	本研究ではこの課題に対し，審判のジェスチャに着目し，位置推定と骨格認識を統合した新たな判別手法を提案する．具体的には，YOLOv8-poseを用いて審判の手首と肩の座標を検出し，3ポイントシュート特有のジェスチャを判別する．提案手法は，位置推定による判別を優先し，未検出時に限りジェスチャの情報を補完的に利用することで，位置推定による判別の欠点を補うアルゴリズムである．
	評価実験の結果，統合手法の正解率は2点シュートで96.95\%，3点シュートで95.12\%を達成した．これは位置推定単体と比較してそれぞれ約7.7ポイント，4.0ポイントの向上である．本結果は，審判のジェスチャを判別することが位置情報の欠損を補う有効な特徴量であり，統合手法がシステムの検出率を向上させることを示している．
}

\maketitle

\chapter{はじめに}
近年，スポーツアナリティクスの分野は著しい発展を遂げている．バスケットボール競技においても，Kubatkoら\cite{kubatko2007}がポゼッションに基づいた効率性指標の重要性を提唱して以来，各選手のパフォーマンスを詳細に表すスタッツを用いた戦術分析や選手評価が一般化している．これらの指標は，シュート成功率，リバウンド数，アシスト数などの定量的データに基づいており，チームの勝利に直結する重要な要素となる．

一方で，現状のスタッツ収集は主に人力で行われている点や，収集に用いるアプリケーションがバスケットボールの競技カテゴリごとに異なり統一されていないなどの課題がある．そのため，スタッツ収集のプロセスを自動化する必要性が高まっている．

この点に関して，Versnikら\cite{stats}は試合映像を入力とし，物体検出や追跡技術を用いてスタッツを自動生成するシステムを提案している．しかし，映像内の選手位置情報のみから，2ポイントシュートか3ポイントシュートかというシュートの種類を判別する手法には課題が存在する．具体的には，足元の位置検出における微小な誤差や，カメラアングルによるホモグラフィ変換の歪みがある場合や，ディフェンス選手などによるオクルージョンが発生し，シュートを打つ選手に対して未検出が起こった場合に，正しくシュートの種類を判別することができないという課題がある．

この課題に対し，本研究では審判のジェスチャに着目する．バスケットボールの審判は，シュートが放たれた際，その放たれた位置や状況に応じて，2ポイントあるいは3ポイントを示す特定のジェスチャを行う規定がある．すなわち，審判の動作は，シュートの種類を決定づける信頼性の高い視覚情報であると言える．

以上の背景から，本研究では，従来の選手位置座標のみに依存した判別手法に加え，シュート時における審判のジェスチャ認識を導入した新たな統合判別手法を提案する．審判のジェスチャを統合することで，上記のような判別困難な状況において，シュート種類の判別精度がどの程度改善されるかを検証し，その有効性を明らかにする．

評価では，シュート種別の判別精度を測るために，システムで推定したシュートの種類と実際の結果を比較する．具体的には，選手位置情報のみを用いた場合，ジェスチャ認識のみを用いた場合，および両者を統合した本手法の3つの条件において実験を行い，それぞれの精度を計測する．

本論文は全5章で構成する．第2章では，バスケットボールに関するコンピュータビジョンの関連研究と本研究の目的について述べる．第3章では，本研究で提案するシュート種類の判別手法の詳細を述べる．第4章では，評価と結果について述べる．第5章では，本研究のまとめについて述べる．

\chapter{コンピュータビジョンを用いた\\バスケットボール映像分析}

\section{バスケットボール競技}
バスケットボールは，1チーム5名のプレイヤーからなる2つのチームが，コートの両端に設置されたリング（バスケット）へボールを投じ，その通過数によって得点を競う球技である ．図2.1に示すコート上には計10名の選手が存在し，攻守を激しく入れ替えながら試合が展開される ．

攻撃側の選手がリングに向けてボールを投じる動作を「シュート」と呼び，その動作を行う選手を「シューター」と呼ぶ ．得点の種類はシュートが放たれた位置によって厳密に区分されており，その基準となるのがゴールの中心から6.75m離れた位置に描かれた「3ポイントライン」である ．図1にバスケットボールのコート図を示す ．このラインを境界として，ラインの内側（2ポイントエリア）から放たれたシュートは「2ポイントシュート」，外側（3ポイントエリア）から放たれたシュートは「3ポイントシュート」と定義される．
また，ゴールの下にある，幅490cm，長さ580cmの長方形の領域を制限区域（ペイントエリア）と呼ぶ．
\begin{figure}[htbp]
	\centering
	% キャプションと画像の間の余白を小さくする（例：5ptにする）
	\setlength{\abovecaptionskip}{1pt}

	% （参考）キャプションとその下の本文の余白を調整したい場合はこちら
	% \setlength{\belowcaptionskip}{0pt}

	\includegraphics[width=0.9\linewidth]{image/court.pdf}
	\caption{バスケットボールコートの図}
	\label{fig:basketball_court}
\end{figure}

試合において反則の判別，および進行管理は，最大3名の審判員（レフェリー）によって行われる ．審判は，ファウルやバイオレーションなどの反則行為に対する判別だけでなく，試合中に発生するあらゆる事象に対して規定のハンドジェスチャを用いて視覚的に伝達を行う役割を担う ．特に，本研究で着目するシュート動作に関しては，シュートが放たれた瞬間にその試行が2ポイントであるか3ポイントであるかを明確にするため，審判はそれぞれに対応した異なるジェスチャを提示する規定となっている ．図2.2にシュートにおける審判のジェスチャを示す ．

\hspace{0.1cm}
\begin{figure}[htbp]
	\centering
	\setlength{\belowcaptionskip}{-0.7em}
	% 高さを揃えるために共通の高さを指定
	\newlength{\imgheight}
	\setlength{\imgheight}{4.0cm} % ← 縮小 (もともと4cm)

	% minipageのオプションは [t] でも [c] でもOK（画像自体が揃うため）
	\begin{minipage}[t]{0.30\linewidth}
		\centering
		% align=c を追加
		\includegraphics[height=1.5\imgheight, align=c]{image/2.jpg}
	\end{minipage}
	\hspace{0.5em}
	\begin{minipage}[t]{0.30\linewidth}
		\centering
		% align=c を追加
		\includegraphics[height=1.5\imgheight, align=c]{image/3.png}
	\end{minipage}
	\caption[2ポイントシュート（左）および3ポイントシュート（右）のジェスチャ]{2ポイントシュート（左）および3ポイントシュート（右）のジェスチャ\cite{gesture}}
	\label{fig:shoot_gesture}
\end{figure}


また，現代のバスケットボール競技においては，試合の勝敗だけでなく，各選手の貢献度やパフォーマンスを詳細に可視化するために「スタッツ（Stats）」と呼ばれるデータが重要視されている．スタッツとは，その選手が何本シュートを決めたか，何回反則を犯したかなどの詳細なプレイ記録の総称である ．図2.3にスタッツの例を示す．表中の数値は選手の成績を表しており，例えば図中の「MIN」の個所は出場時間，「A（Attempted）」の箇所はシュートを打った数，「M（Made）」の箇所はシュートを決めた数を示している ．現状，これらのスタッツ収集は，専門の記録員が収集用のシステムを用いて行っており，依然として人の目視による判断と手動入力に依存している．
しかし，このような人手による記録作業は記録員への負担が大きく，リアルタイムなデータ提供やヒューマンエラーの排除には限界がある．そのため，そのため，近年ではコンピュータビジョン等の技術を用いて，これらの記録プロセスを自動化する試みが行われている．特に，スタッツの判別には得点やファウルといった審判の判定結果が直結するため，プレイの認識だけでなく，審判のジェスチャを自動認識する技術も重要な要素となる．

\begin{figure}[htbp]
	\centering
	\setlength{\belowcaptionskip}{-0.7em}

	% 画像ファイルの幅を、本文の幅の80%に設定（数字を変えて調整可能）
	\includegraphics[width=1.0\linewidth]{image/stats.pdf}

	\caption{スタッツの例}
	\label{fig:stats}
\end{figure}

\section{関連研究}
本節では，バスケットボールにおけるスタッツの自動収集および審判のジェスチャ認識に関する既存研究を概観し，それぞれの提案手法の詳細と課題について述べる．

\subsection{スタッツの自動収集}
VeršnikとŠajnは，NBAの放送映像を入力とし，ディープラーニングを用いて選手やボールを検出し，試合の統計データ（スタッツ）を自動的に収集するシステムを提案している\cite{stats}． 彼らの手法では，まず物体検出アルゴリズムであるYOLOを用いて選手を検出し，追跡アルゴリズムであるDeepSORTと組み合わせることで各選手のトラッキングを行っている．さらに，DeepSORT単体では選手同士が交差した際などに選手を判別するIDが入れ替わりチーム分類を誤る問題があったため，MobileNetV2を用いた画像分類モデルを導入し，チーム分類の精度を99.41\%まで向上させた．シュート判別においては，ホモグラフィ変換を用いて映像内の座標をコートの座標へ変換し，ボールとゴールの位置関係および選手の足元の位置に基づいて，3ポイント，2ポイント，フリースローの判別を行っている．

しかしながら，本手法にはシュート判別の精度が低いという課題がある．彼らの実験結果によると，シュート試投数の検出精度は高いものの，シュート成功の判別や種別の分類において誤検出が目立った．具体的には，3ポイントシュートの成功検出精度は53.4\%に留まり，試投の検出精度も63.3\%であった．この精度の低さは，主にホモグラフィ変換の不安定さに起因している．特にコートの片側における変換行列の計算誤差が大きく，フリースローの場面において，選手の位置が3ポイントラインの外側であると誤って計算され，3ポイントシュートとして誤分類される事例が多発したことが報告されている．このように，ホモグラフィ変換に依存したシュート判別は，カメラアングルの変化やオクルージョンに対して脆弱であり，実用的なスタッツ収集には不十分である．

\subsection{HOG，LBP特徴量とSVMによる審判ジェスチャ認識}
Žemgulysらは，バスケットボールの試合映像を対象に，コンピュータビジョンを用いて，時計の停止・スリーポイント・ファウルという3種類の審判ジェスチャを識別する手法を提案している\cite{zemgulys}．彼らは，スポーツ映像のようなノイズの多い実環境下での認識を実現するため，画像の前処理としてRGB画像をグレースケール化し，Sobelフィルタを用いてエッジ検出を行うことで，照明変動の影響を軽減している．特徴量抽出には，形状情報を捉えるHOGと，テクスチャ情報を捉えるLBPの2種類を採用し，これらをSVMおよびランダムフォレストで分類する実験を行った．実験の結果，LBP特徴量とSVMを組み合わせたモデルが最も高い性能を示し，全クラス平均で95.6\%の認識精度を達成した．特に3ポイントシュート成功のジェスチャに関しては，正面からの映像で97.7\%という極めて高い精度で認識できている．

しかしながら，この研究の課題は，認識したジェスチャをスタッツの自動集計やシュート判別の結果確定に活用していない点にある．Žemgulysら研究の主眼は，あくまで審判と記録員間のコミュニケーションミスを減らすためのジェスチャの認識そのものに置かれている．そのため，認識された3ポイント成功のジェスチャが，実際の得点加算処理や，Veršnikらの研究で課題とされたようなシュートが本当に入ったかどうかの判別の補正に利用されるシステム構成にはなっていない．単独のジェスチャ認識に留まっており，試合のコンテキストと統合されたスタッツ収集システムとしては機能していない．

\subsection{軽量AIモデルGIS-ResTを用いたリアルタイム審判ジェスチャ認識}
Wangらは，バスケットボールの試合映像から，審判のジェスチャをリアルタイムで認識するシステムを提案している\cite{wang}．

彼らが開発した，GIS-ResTという軽量なAIモデルは，挙手など一瞬の素早い動きから，反則が起こった後のジェスチャなどの一連の長い動作まで，時間の長さが異なる動きの特徴を同時に捉える仕組みを採用している． さらに映像全体を認識し，局所的な特徴の相関関係や，最も顕著に現れる動きの特徴といった，判別の決め手となる重要な情報を強調する仕組みを組み合わせることで，認識の精度を高めている．

この手法は，従来の3D畳み込み手法に比べて計算量を大幅に減らしながらも，ジェスチャの細かい特徴まで捉えることに成功しており，92.20\%という高い平均認識率を達成した．また，このシステムはスマホやWebブラウザでも動くように設計されており，認識した判定結果を音声でユーザー（選手や観客）に知らせる機能も備えている．

この研究もまた，スタッツの収集やシュート判別にジェスチャを利用していない点が課題である．本システムの目的は，聴覚や視覚に障害を持つ観客への支援や，記録員が見逃した判別の確認，あるいはコーチや選手がビデオ分析を行う際の該当シーンの検索補助に限定されている．システムはジェスチャを分類して提示する補助機能として設計されており，例えば，審判が2点のジェスチャをしたから，自動的にスコアに2点を加算するといった，スタッツ収集の自動化に直結する機能は実装されていない．

\section{研究の目的}
前節で述べたように，Veršnikら\cite{stats}による先行研究では，ホモグラフィ変換を用いて映像内の座標をコート平面上の座標へ変換し，その位置情報に基づいてシュート種別の判別を行っていた．しかし，この手法はカメラアングルやレンズの歪みの影響を受けやすく，特にコートの奥側や端部において変換行列の計算誤差が増大するという欠点を抱えていた．その結果，本来はフリースローである事象を3ポイントシュートとして誤認識するなど，シュート種類の判別精度が低下する課題が未解決のまま残されている．

一方で，バスケットボールの競技規則において，審判は3ポイントシュートの試投時および成功時に，2ポイントシュートは成功時のみに，その種類を特定するための明確なジェスチャを行うことが義務付けられている．Žemgulysら\cite{wang}やWangら\cite{zemgulys}の研究が示すように，画像処理によるジェスチャ認識は高い精度を実現しているものの，これらをシュート種類の判別に使用する試みはなされていない．

そこで本研究では，従来の位置情報に基づく判別に加え，シュート時における審判のジェスチャ認識を新たな判断パラメータとして導入する手法を提案する． 本研究の目的は，先行研究\cite{stats}のアプローチに対して審判のジェスチャ情報を統合することで，シュート種類の判別精度がどの程度向上するかを明らかにすることである．具体的には，シュート位置による判別のみを用いた場合と，ジェスチャ認識による判別を組み合わせた場合とでシュート判別の検出率を比較検証し，シュート種類判別システムにおけるジェスチャ認識の有効性を検討する．

\chapter{審判のジェスチャを利用した\\シュート種類判別法}
\section{概要}
本節では，提案手法の処理概要について述べる．本手法の処理の流れを図3.1に示す．本手法は，バスケットボールのシュートシーンを撮影した動画データと，その動画内でシュートが放たれた瞬間のフレーム番号を入力とする．

本手法では，大きく分けて二つのアプローチによる判別処理が行われる． 第一に，入力動画全体に対して審判の検出および骨格認識を行い，そのジェスチャに基づいてシュートの種類を判別する処理である． 第二に，指定されたシュート瞬間のフレームに対し物体検出を行い選手を検出し，ホモグラフィ変換を用いて画像上の選手の座標をコート平面上の座標へ変換する．その後，シューターを選択し，選択したシューターの変換後の位置情報に基づいたシュート種類の判別を行う処理である．

最終的な判別においては，これら二つの結果を統合する．基本的には選手の位置情報に基づく判別を行うが，選手の未検出により位置特定が失敗となった場合に対し，審判のジェスチャ認識による結果を補完的に採用する．この統合処理を経て，最終的なシュート種別を出力する．

\begin{figure}[H]
	\centering
	\setlength{\belowcaptionskip}{-0.7em}

	% 画像ファイルの幅を、本文の幅の80%に設定（数字を変えて調整可能）
	\includegraphics[width=1.0\linewidth]{image/system.pdf}

	\caption{処理の流れ}
	\label{fig:system}
\end{figure}

\section{選手検出}
本手法における映像内のオブジェクト検出および骨格認識には，Ultralytics社が開発したYOLO (You Only Look Once) \cite{detect} を用いる．YOLOは，画像全体を一度の処理で解析することで，対象物の位置とクラスを同時に推定可能なアルゴリズムであり，高速かつ高精度なリアルタイム処理を実現している．

YOLOのデフォルトモデルによる実行例を図\ref{fig:yolo_default}に示す．図中の枠（バウンディングボックス）に付随する文字は検出された物体のクラス名を，数値はその検出の信頼度を表している．

\begin{figure}[H]
	\centering
	\setlength{\belowcaptionskip}{-0.8em}
	% 画像ファイル名を指定
	\includegraphics[width=0.8\linewidth]{image/basketball.jpg}
	\caption{YOLOによる物体検出の実行例}
	\label{fig:yolo_default}
\end{figure}

本手法における具体的な実装をCode~\ref{detect}に示す．このプログラムは画像を入力とし，検出した物体の識別ID，信頼度，および認識した領域（バウンディングボックス）の座標を出力するものである．

\begin{minipage}{14cm}
	\lstinputlisting[caption = 物体検出のソースコード ,label = detect]{code/detect.py}
\end{minipage}

さらに，YOLOにはデフォルトモデルの利用に加え，ユーザー独自のデータセットを用いて学習を行えるという特徴がある．具体的には，ユーザーが用意した画像データに対して独自のアノテーションを行い，それを学習させることで，特定の対象物を認識する専用モデルを作成できる．

本手法ではこの機能を活用し，標準のモデルに加えて，バスケットボールの試合映像において審判とゴール　を正確に検出・分類するための専用モデルを構築し，利用している．
\section{ホモグラフィ変換}
ホモグラフィ変換とは，ある平面上の点を別の平面上の対応する点へ写像する変換の一種である．
画像処理の分野においては，カメラの視点変更による画像の幾何学的歪みを補正する際や，異なる視点で撮影された複数の画像を位置合わせする際によく用いられる．

また，ホモグラフィ変換は以下に示すように，同次座標系における$3 \times 3$の行列演算として定式化される．

\begin{equation}
	\label{eq:homography}
	\begin{pmatrix}
		wx' \\
		wy' \\
		w
	\end{pmatrix}
	=
	\begin{pmatrix}
		a & b & c \\
		d & e & f \\
		g & h & i
	\end{pmatrix}
	\begin{pmatrix}
		x \\
		y \\
		1
	\end{pmatrix}
\end{equation}

小枝ら\cite{opencv_book}によれば，ホモグラフィ変換は，2次元画像が3次元空間内の平面上に存在すると仮定した際，それを様々な視点から観測することで得られる変換であるとされる．

したがって，平面状の物体であれば，視点が異なる画像間での対応関係をこの変換によって記述することができる．

\section{シューター位置による判別}
本手法では，審判のジェスチャ認識と並行して，画像処理を用いた選手の位置推定によるシュート種別の判別を行う．本節では，ホモグラフィ変換を用いた座標変換および判別アルゴリズムの詳細について述べる．

\subsection{ホモグラフィ変換行列の算出}
\subsubsection{ペイントエリアの抽出と特徴点の検出}
まず，入力フレームをHSV色空間へ変換し，指定された色範囲に基づき二値化処理を行うことでペイントエリアを抽出する．ノイズ除去のためのクロージングおよびオープニングを1回適用した後，ペイントエリアの領域特定を行う．

ここで，単純に最大面積の輪郭を抽出するだけでは，選手がペイントエリアの境界線上に重なることなどで領域が分断された際，誤検出が生じる可能性がある．そこで本手法では，以下の手順によりロバストな領域特定を行っている．

\begin{enumerate}
	\item \textbf{微小領域の除去:} \\
	      検出された全輪郭のうち，面積が閾値未満のものはノイズとみなし除外する．

	\item \textbf{近接輪郭の統合:} \\
	      残った輪郭群に対し，互いの距離を計算する．ある輪郭と別の輪郭の距離が一定以下である場合，それらは同一の物体（ペイントエリアの一部）であると判定する．これらをグループ化し，その全頂点を包含する凸包（Convex Hull）を生成することで，分断された領域を一つの閉領域へと復元する．

	\item \textbf{最大面積領域の選出と時間的安定化:} \\
	      統合処理によって得られた複数の閉領域候補の中から，その面積 $S_{current}$ が最大となる輪郭 $C_{current}$ を選出する．
	      ここで，選手がカメラとペイントエリアの間を横切る遮蔽（オクルージョン）や，照明環境の変化により，検出される領域が一時的に欠損し，面積が急激に減少する場合がある．これを防ぐため，過去のフレームにおいて検出された最大面積 $S_{stable}$ を保持し，現在の面積 $S_{current}$ と比較を行う．
	      具体的には，式(\ref{eq:peak_hold})に基づいて更新判定を行う．

	      \begin{equation}
		      \label{eq:peak_hold}
		      \text{if } S_{current} \ge S_{stable} \quad \text{then} \quad
		      \begin{cases}
			      C_{stable} \leftarrow C_{current} \\
			      S_{stable} \leftarrow S_{current}
		      \end{cases}
	      \end{equation}

	      すなわち，より面積が大きい領域が検出された場合のみ情報を更新し，遮蔽等により面積が減少した場合は，更新を行わず過去の最も大きい領域 $C_{stable}$ を継続して使用する．これにより，フレーム間の検出ブレを抑制している．
\end{enumerate}

以上の処理により特定された輪郭に対し，上下左右の点すなわち画像座標系における $x, y$ 座標の最大値・最小値を持つ以下の4点を検出する．

\begin{itemize}
	\item 最左点 (Leftmost): $x$座標が最小となる点
	\item 最右点 (Rightmost): $x$座標が最大となる点
	\item 最上点 (Topmost): $y$座標が最小となる点
	\item 最下点 (Bottommost): $y$座標が最大となる点
\end{itemize}

これを入力画像上の点集合 $\mathbf{P}_{src}$ とする．

\subsubsection{ゴールの位置推定と座標の対応付け}
画像上の特徴点が，実際のコートのどの位置に対応するかは，カメラがコートの左右どちら側から撮影しているかによって異なる．そこで本手法では，YOLOを用いて検出されたゴールのバウンディングボックスの中心座標 ($x_{goal}$) と，抽出したペイントエリアの中心座標 ($x_{paint}$) の水平位置関係を用いて，対応付けを動的に切り替えるアルゴリズムを実装した．

\begin{enumerate}
	\item \textbf{ゴールが右側にある場合 ($x_{goal} > x_{paint}$):} \\
	      カメラはコートの左側から右方向を見ていると判断される．この場合，画像上の「最左点」はフリースローラインの右端（奥側），「最右点」はエンドラインの左端（手前側）に対応付けられる．

	\item \textbf{ゴールが左側にある場合 ($x_{goal} \le x_{paint}$):} \\
	      カメラはコートの右側から左方向を見ていると判断される．この場合，画像上の「最左点」はエンドラインの右端（奥側），「最右点」はフリースローラインの左端（手前側）に対応付けられる．
\end{enumerate}

実際のコート寸法として，制限区域（ペイントエリア）の幅 490cm，長さ 580cm を定義し，これを変換先の座標集合 $\mathbf{P}_{dst}$ とした．
最終的に，決定された $\mathbf{P}_{src}$ と $\mathbf{P}_{dst}$ のペアを用いて，OpenCVの \texttt{cv2.findHomography} 関数により，$3 \times 3$ のホモグラフィ行列 $\mathbf{H}$ を算出する．
\begin{figure}[H]
	\centering
	\setlength{\belowcaptionskip}{-0.5em}
	% 画像ファイル名は実際のファイル名（例: image/select_points.pdf）に変更してください
	\includegraphics[width=0.8\linewidth]{image/4points_select1.png}
	\caption{ホモグラフィ変換のための4点認識の様子}
	\label{fig:4points_select}
\end{figure}

\subsection{選手座標の推定とシュート判別}
前節で構築したYOLOモデル，あるいは汎用の物体検出モデルを用いて検出された選手のバウンディングボックス情報を利用し，以下の手順で判別を行う．

\begin{enumerate}
	\item \textbf{足元座標の取得}:
	      検出された選手のバウンディングボックスの左上座標を $(x_1, y_1)$，右下座標を $(x_2, y_2)$ とする．このとき，選手がコートに接している位置（足元）の画像座標 $(u, v)$ を，バウンディングボックスの下辺中央として次式で定義する．
	      \begin{equation}
		      (u, v) = \left( \frac{x_1 + x_2}{2}, \ y_2 \right)
	      \end{equation}

	\item \textbf{座標変換}:
	      算出した行列 $\mathbf{H}$ を用いて，画像座標 $(u, v)$ をコート平面上の座標 $(wx', wy')$ へ変換する．変換式は以下の通りである．
	      \begin{equation}
		      \label{eq:homography}
		      \begin{pmatrix}
			      wx' \\
			      wy' \\
			      w
		      \end{pmatrix}
		      =
		      \begin{pmatrix}
			      a & b & c \\
			      d & e & f \\
			      g & h & i
		      \end{pmatrix}
		      \begin{pmatrix}
			      u \\
			      v \\
			      1
		      \end{pmatrix}
	      \end{equation}
	\item \textbf{シューターの特定}:
	      本手法では，画像内に複数の選手が存在する場合の誤判別を防ぐため，マウスイベントを用いたインタラクティブな選択手法を採用した．システム利用者が画像上の任意の選手をクリックすることで，その座標に対応する選手IDをシューターとして特定する．

	\item \textbf{シュート種別の判別}:
	      特定されたシューターの変換後座標 $(x', y')$ が，3ポイントラインの内側（2点エリア）にあるか外側（3点エリア）にあるかを判別する．
	      図\ref{fig:3point_judge}に示すように，バスケットボールの3ポイントラインは，ゴールを中心とした円弧部分（トップ）と，サイドラインに平行な直線部分（コーナー）の2つの幾何学的要素から構成される．

	      本手法では，この形状的特徴に基づき領域を分割して判定を行うため，以下のコートパラメータを定義する．
	      \begin{itemize}
		      \item ゴール中心座標: $(c_x, c_y)$
		      \item 3ポイントラインの半径: $R$
		      \item コーナー直線のX座標範囲: $[x_{L}, x_{R}]$
		      \item 直線と円弧の接続点Y座標: $y_{int}$
	      \end{itemize}

	      判別処理では，まずシューターのY座標 $y'$ と接続点 $y_{int}$ を比較してエリアを特定し，図\ref{fig:3point_judge}に示すように各エリアの形状に応じた条件分岐を行う．

	      \begin{itemize}
		      \item \textbf{コーナーエリア ($y' \le y_{int}$) の場合}:
		            ベースラインから接続点 $y_{int}$ までの領域では，3ポイントラインは直線となる（図\ref{fig:3point_judge}の緑色斜線部参照）．
		            したがって，シューターのX座標が直線の範囲内にあるか（$x_{L}$ と $x_{R}$ の間か）によって判定を行う．
		            \begin{equation}
			            \text{Result} =
			            \begin{cases}
				            \text{IN (2pt)}  & \text{if } x_{L} \le x' \le x_{R} \\
				            \text{OUT (3pt)} & \text{otherwise}
			            \end{cases}
		            \end{equation}

		      \item \textbf{トップエリア ($y' > y_{int}$) の場合}:
		            接続点 $y_{int}$ より奥の領域では，3ポイントラインは円弧となる（図\ref{fig:3point_judge}の青色斜線部参照）．
		            ゴール中心からのユークリッド距離の二乗 $d^2 = (x' - c_x)^2 + (y' - c_y)^2$ を計算し，半径の二乗 $R^2$ と比較することで判定を行う．
		            \begin{equation}
			            \text{Result} =
			            \begin{cases}
				            \text{IN (2pt)}  & \text{if } d^2 \le R^2 \\
				            \text{OUT (3pt)} & \text{otherwise}
			            \end{cases}
		            \end{equation}
	      \end{itemize}

	      \begin{figure}[H]
		      \centering
		      \setlength{\belowcaptionskip}{-0.8em}
		      % 画像ファイル名を指定
		      \includegraphics[width=0.9\linewidth]{image/3point_judge.png}
		      \caption{スリーポイントシュートの判定領域モデル}
		      \label{fig:3point_judge}
	      \end{figure}
\end{enumerate}

この処理の実装結果として，変換後の位置を可視化した例を図\ref{fig:hom_court}に，判別結果を図\ref{fig:hom_select}に示す．
なお，図\ref{fig:hom_court}において，3ポイントラインの外側に位置する選手は赤色の円，内側に位置する選手は青色の円で表示されている．
また，図\ref{fig:hom_select}においては，選択した選手のバウンディングボックスが黄色く表示され，その上部に判別結果が表示されている．

\begin{figure}[H]
	\centering
	\setlength{\belowcaptionskip}{-0.5em}
	% コート変換後の位置
	\includegraphics[width=0.5\linewidth]{image/hom_court.png}
	\caption{コート平面上への選手位置の投影結果}
	\label{fig:hom_court}
\end{figure}

\begin{figure}[H]
	\centering
	\setlength{\belowcaptionskip}{-0.5em}
	% 選択画面（判別結果が表示されているもの）
	\includegraphics[width=0.8\linewidth]{image/hom_select.png}
	\caption{選手選択および判別の実行例}
	\label{fig:hom_select}
\end{figure}

\section{審判検出モデルの構築}
本手法では，バスケットボールの試合映像において審判を正確に認識するため，YOLOを用いた専用モデルの作成を行った．本節では，その学習に使用したデータセットおよび学習結果について述べる．

\subsection{データセットとアノテーション}
本手法の学習および評価に用いるデータセットとして，YouTube上で公開されている日本プロバスケットボールリーグ（Bリーグ）の試合映像2試合分 \cite{kawasaki,chiba} を利用した．

データセットの構築にあたり，映像内からシュート動作におけるジャンプ踏み切り直前の瞬間を「シュートのフレーム」と定義し，計254枚の静止画フレームを抽出した．また，各シュートに対し，その直前のパスからシュートが放たれた後，リバウンドを確保するかゴールが成功するまでの一連の区間を「シュートの動画」として同数切り出し，保存した．

審判検出モデルの学習には，前述の「シュートのフレーム」254枚を使用した．教師データの作成には，オープンソースの画像アノテーションツールであるLabelImg\cite{labelimg}を用いた．

\subsection{学習結果}
作成したデータセットを用いてYOLOの学習を行った．
学習済みモデルの評価指標としてmAPを用いた結果，mAP@0.5は19.5\% ，mAP@0.5:0.95は12.1\% であった．
一般的な物体検出タスクの基準と比較してこれらの値は低い水準に留まっているが，これは学習に用いたデータセットが254枚と非常に小規模であったことに起因すると考えられる．
しかしながら，本研究における物体検出の主目的は，後段の骨格認識処理のために審判の大まかな位置を特定することにある．
実際の検出結果を確認したところ，ジェスチャが未検出であったケースが一つもなかったため，本システムの目的においては許容できる精度であると判断した．

\section{審判の骨格認識}
本手法では，検出された審判のジェスチャに基づいてシュートの種類を判別するため，骨格認識を行う．
骨格認識モデルには，物体検出と同様にYOLOシリーズの \texttt{yolov8x-pose} \cite{bone} を用いる．これは，YOLOの高精度な検出能力を維持しつつ，人物の姿勢推定を同時に行うことが可能なモデルである．

このモデルを用いることで，画像内の人物領域から「キーポイント」と呼ばれる関節の座標を取得することができる．取得可能なキーポイントは，鼻，目，耳，肩，肘，手首，腰，膝，足首の計17箇所であり，各点に対して画像上の座標 $(x, y)$ および検出の信頼度が算出される．
YOLOによって定義されるキーポイントのインデックスと対応する身体部位の一覧と実行結果の例を図\ref{fig:pose_keypoints}に示す．

本研究では，これらのキーポイントのうち，特に上半身（肩，肘，手首）の座標位置関係に着目し，その形状パターンを解析することで審判のシグナルを識別する．
\begin{figure}[H]
	\centering
	\setlength{\belowcaptionskip}{-0.5em}
	% 選択画面（判別結果が表示されているもの）
	\includegraphics[width=0.8\linewidth]{image/keypoints-crop.pdf}
	\caption{YOLOによる骨格認識のキーポイント一覧と実行例}
	\label{fig:pose_keypoints}
\end{figure}

\begin{minipage}{14cm}
	\lstinputlisting[caption = 骨格認識のソースコード ,label = bone]{code/bone.py}
\end{minipage}

Code~\ref{bone}は，骨格認識のソースコードであり．画像を入力とし，検出した人数，関節の座標と信頼度を出力としている．

\section{ジェスチャ認識を用いた判別}
前節で述べた骨格認識によって得られたキーポイント情報を用い，最終的なシュート種類の自動判別を行う．

バスケットボールの競技規則において，3ポイントシュートの試投時には，図\ref{fig:3gesture_t}に示すように，審判は片手を頭上に挙げるジェスチャを行う．一方，2ポイントシュートの場合，通常この動作は行われず，腕を下ろした状態である．本システムではこの差異に着目し，以下の手順で判別を行う．

\begin{figure}[H]
  \centering
  \setlength{\belowcaptionskip}{-0.5em}

  % --- 左側の画像 ---
  % minipageの幅を広げて揃える
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    % 画像幅をminipageいっぱい(1.0)に指定して大きさを揃える
    \includegraphics[width=0.5\linewidth]{image/3.png}
    % \caption{キーポイント一覧}
  \end{minipage}
  % \hfill をやめて狭いスペースを指定する
  \hspace{0.02\linewidth}
  % --- 右側の画像 ---
  % minipageの幅を広げて揃える
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    % 画像幅をminipageいっぱい(1.0)に指定して大きさを揃える
    \includegraphics[width=1.0\linewidth]{image/3pt.pdf}
    % \caption{骨格認識の結果}
  \end{minipage}

  \caption{YOLOによる骨格認識のキーポイント一覧と実行例}
  \label{fig:pose_keypoints}
\end{figure}

\subsection{判別条件の定義}
YOLOv8-poseによって取得される17点のキーポイントのうち，本手法では上半身の動作に関わる肩と手首 の座標を利用する．
画像座標系において，原点は画像の左上に位置し，鉛直下向きに $y$ 軸が定義される．したがって，画面上で「手が肩より上にある」状態は，手首の $y$ 座標の値が肩の $y$ 座標の値よりも小さい状態として定義できる．

具体的には，検出された任意の審判について，左手首の座標を $y_{lw}$，左肩の座標を $y_{ls}$，右手首の座標を $y_{rw}$，右肩の座標を $y_{rs}$ としたとき，以下の式(\ref{eq:judge_3pt})のいずれかが成立した場合に「3ポイントシュートのジェスチャ」であると判別する．

\begin{equation}
	y_{lw} < y_{ls} \quad \lor \quad y_{rw} < y_{rs}
	\label{eq:judge_3pt}
\end{equation}

\subsection{動画全体を通した判別フロー}
実際の試合映像では，審判が最大で3人存在する可能性があるため，本システムでは検出されたすべての審判（最大3人）を対象に判別を行う．
判別フローは以下の通りである．

\begin{enumerate}
	\item 対象となる「シュートの動画」の全フレームに対して処理を行う．
	\item 各フレームにおいて，検出された全ての審判について式(\ref{eq:judge_3pt})の条件を確認する．
	\item 動画内のいずれか1フレームでも，いずれかの審判において条件が満たされた場合，そのシュートを「3ポイントシュート」と判別する．実際に3ポイントシュートの条件を満たし判別が行われたフレームの例を図\ref{fig:3gesture}に示す．
	\item 動画の全フレームを通して一度も条件が満たされなかった場合，そのシュートを「2ポイントシュート」と判別する．
\end{enumerate}

% --- 図の定義 ---
\begin{figure}[H]
	\centering
	\setlength{\belowcaptionskip}{-0.5em}
	\includegraphics[width=0.75\linewidth]{image/3pt_ges.png}
	\caption{3ポイントと判別するフレーム（腕が挙がっている状態）}
	\label{fig:3gesture}
\end{figure}

\section{統合手法を用いた判別}
本研究では，最終的なシュート種別の判別精度および判別可能率を向上させるため，前述した「シューター位置による判別」と「ジェスチャ認識を用いた判別」を組み合わせた統合手法を提案する．

シューター位置による判別は，3ポイントラインとの幾何学的な位置関係を直接計算するため，原理的な信頼性は高い．
しかし，選手同士の重なりやフレームアウトにより，シューターの検出や足元の特定に失敗し，未検出となる場合が存在する．
実際に，位置推定手法において検出に失敗した例を図\ref{fig:undetected}に示す．同図のような状況では，シューターの足元座標が取得できないため位置による判別は不可能となる．

一方で，ジェスチャ認識を用いた判別は，シューターの位置検出に依存せずに判別が可能であるという利点を持つ．
そこで統合手法では，以下の優先順位に従って最終的な判別を出力する．

\begin{enumerate}
	\item まず，シューター位置による判別による判別を試みる．
	\item シューター位置による判別により「2ポイント」または「3ポイント」と判別された場合，その結果を最終的な判別として採用する．
	\item シューター位置による判別の結果が未検出であった場合に限り，ジェスチャ認識を用いた判別の結果を採用する．
\end{enumerate}

このアルゴリズムにより，シューター位置による判別が高い精度で判別できるシーンではその正確さを活かしつつ，位置特定が困難なシーンにおいてはジェスチャ認識を用いた判別によって判別を補完する．

% --- 図の挿入 ---
\begin{figure}[H]
	\centering
	\setlength{\belowcaptionskip}{-0.5em}
	\includegraphics[width=0.9\linewidth]{image/NG.png}
	\caption{位置推定手法において未検出となるフレームの例}
	\label{fig:undetected} % ← ラベル名を重複しないものに変更しました
\end{figure}

\chapter{評価}
\section{評価の目的}
本章では，第3章で提案したシュート種別自動判別システムの有効性を検証するために行った評価について述べる．

前章で述べた通り，シュートの位置に基づく判別手法は，原理的な信頼性は高いものの，オクルージョン等による未検出が発生するという課題があった．一方，審判のジェスチャ認識に基づく手法は，選手の位置に依存せずに判別可能であるが，映像解析上のノイズやほかのジェスチャなどを誤検出するリスクを含んでいる．

そこで本実験では，Bリーグのバスケットボール試合映像 \cite{kawasaki,chiba} を用いて，以下の3点について評価を行うことを目的とする．

\begin{enumerate}
	\item \textbf{位置推定手法の限界の確認}: 従来のアプローチである位置推定のみを用いた場合，どの程度の割合で未検出が発生するかを確認する．
	\item \textbf{ジェスチャ認識の精度の検証}: 審判の骨格情報を用いたジェスチャ認識が，実用的な精度で2ポイント/3ポイントを識別可能か検証する．
	\item \textbf{統合手法の有効性の実証}: 位置推定とジェスチャ認識を組み合わせた提案手法（統合手法）を用いることで，判別精度を維持しつつ，システム全体の判別可能率（カバー率）が向上することを明らかにする．
\end{enumerate}

\section{評価の方法}
本実験では，提案手法の有効性を検証するため，以下のデータセットおよび条件を用いて評価を行う．

\subsection{使用データセット}
評価には，前章で作成した計254本のシュート動画を用いる．
データセットの内訳は，2ポイントシュートが \textbf{131}本，3ポイントシュートが \textbf{123}本である．
これらの動画に対する正解ラベルは，筆者が実際の映像を目視で確認し，バスケットボールのルールに基づいて付与したものを使用する．

\subsection{比較対象とする手法}
各アプローチの特性と統合による効果を明らかにするため，以下の3つの手法についてそれぞれ評価を行う．

\begin{enumerate}
	\item \textbf{シュート位置による判別}:
	      シューターの足元座標と3ポイントラインの幾何学的位置関係のみを用いて判別を行う．オクルージョン等により足元座標が取得できない場合は「未検出」として扱う．

	\item \textbf{ジェスチャ認識による判別}:
	      審判の骨格認識（YOLOv8-pose）を用いて検出された腕の動作のみを用いて判別を行う．

	\item \textbf{統合手法による判別（提案手法）}:
	      上記2つを組み合わせた手法である．基本的にはシュート位置による判別を優先し，位置推定において「未検出」となった場合に限り，ジェスチャ認識の結果を採用して判別を行う．
\end{enumerate}

\subsection{評価基準}
評価基準として，システムが出力した判別結果が，入力されたシュート動画の実際のアクションと一致するかを検証する．
本実験では，システムが出力した判別結果が正解ラベルと一致したものを「検出」とした．

これに基づき，全データに対する検出率および，誤検出率と未検出率を算出し，手法間の比較を行う．

\section{結果}
作成したデータセットに対する，シュート位置による判別，ジェスチャ認識による判別，およびそれらを組み合わせた統合手法による判別結果を表\ref{tab:result_comparison}に示す．

\begin{table}[htbp]
	\centering
	\caption{各手法による判別結果の比較}
	\label{tab:result_comparison}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{c|c|ccc}
			\hline \hline
			\textbf{判別の種類}                    & \textbf{シュートの種類} & \textbf{検出}   & \textbf{未検出} & \textbf{誤検出} \\ \hline

			\multirow{2}{*}{\textbf{シューター位置}} & 2ポイント            & 116 (89.23\%) & 14 (10.77\%) & 1 (0.77\%)   \\ \cline{2-5}
			                                  & 3ポイント            & 113 (91.13\%) & 5 (4.03\%)   & 4 (3.23\%)   \\ \hline

			\multirow{2}{*}{\textbf{ジェスチャ}}   & 2ポイント            & 86 (66.15\%)  & 0 (0.00\%)   & 45 (34.62\%) \\ \cline{2-5}
			                                  & 3ポイント            & 110 (88.71\%) & 0 (0.00\%)   & 13 (10.48\%) \\ \hline

			\multirow{2}{*}{\textbf{統合}}      & 2ポイント            & 127 (96.95\%) & 0 (0.00\%)   & 4 (3.05\%)   \\ \cline{2-5}
			                                  & 3ポイント            & 117 (95.12\%) & 0 (0.00\%)   & 6 (4.88\%)   \\ \hline \hline
		\end{tabular}
	}
\end{table}

まず，各単体手法の結果について述べる．
シュート位置による判別について，2ポイントシュートでは検出率が89.23\%，未検出率が10.77\%，誤検出率が0.77\%であり，3ポイントシュートでは検出率が91.13\%，未検出率が4.03\%，誤検出率が3.23\%であった．
一方，ジェスチャ認識による判別について，未検出は発生しなかった．また，2ポイントシュートでは検出率が66.15\%，誤検出率が34.62\%であり，3ポイントシュートでは検出率が88.71\%，誤検出率が10.48\%であった．

次に，提案する統合手法の結果について述べる．
統合手法では，2ポイントシュートの検出率が96.95\%，3ポイントシュートの検出率が95.12\%となった．
これは，シュート位置による判別単体の場合と比較して，2ポイントで約7.7ポイント，3ポイントで約4.0ポイントの向上である．
この値は，Versnikらの研究\cite{stats}と比較しても高い値である．
統合手法の判別結果がシュート位置による判別結果と比較し，ともに向上したことは，シュート位置による判別における未検出のケースを，ジェスチャ認識により効果的に補完しているといえる．

\section{考察}
本実験の結果より，ジェスチャ認識を用いた判別においては，3ポイントシュートの検出に関しては一定の成果が得られたものの，2ポイントシュートに対しては誤検出率が高い傾向が見られた．また，一部の3ポイントシュートにおいてもジェスチャ認識に失敗する事例が確認された．
本節では，これらの要因について「判別アルゴリズムの特性」および「画像認識プロセスにおける技術的課題」の二つの観点から詳細に考察する．

\subsection{判別アルゴリズムと誤検出の要因}
本来は2ポイントであるにも関わらず，誤って3ポイントと判別されたことによる2ポイントシュートの誤検出率が高くなった最大の要因は，本手法で採用した判別手法の単純性と，バスケットボール競技特有の審判動作の多様性にあると考えられる．

本研究におけるジェスチャ判別は，骨格推定によって得られた「手首」と「肩」の$y$座標を比較し，手首が肩よりも上位にあるか否かという条件のみに基づいている．
しかし，実際のバスケットボールの試合において，審判が腕を肩より上に挙げる動作は3ポイントシュートの試投に限られない．例えば，ファウルの宣告，反則の発生によるクロックの停止，タイムアウトの合図，選手交代の許可，あるいはスローインの方向指示など，試合運営上の多くの局面で挙手動作が行われる．
片腕または両腕の垂直挙上によって表す3ポイントシュートのジェスチャは，これらの他のシグナルと形状的に類似しており，手と肩の位置関係のみを用いた単純な閾値処理では，これらを明確に識別することは困難である．
このような誤検出を根本的に抑制するためには，単なる姿勢推定だけでなく，試合状況を考慮し，その場面がシュートに関連する局面であるかを識別するシーン認識技術の実装が不可欠である．

\subsection{物体検出および骨格推定の精度に起因する課題}
次に，3ポイントシュートであるにも関わらずジェスチャ認識に失敗し，誤検出となったケースについて，その技術的な要因を考察する．

第一の要因として，物体検出におけるバウンディングボックスの領域欠損が挙げられる．
作成した審判検出モデルは，主に直立あるいは走行中の審判を学習データとして構築されている．しかし，審判が3ポイントシュートの合図として腕を高く真上に伸ばした際，推論されたバウンディングボックスが審判の頭頂部付近までしかカバーできず，最も重要な特徴量である「手首から先の領域」が枠外にはみ出してしまう現象が散見された．検出領域内に手首が存在しない場合，後段の骨格推定モデルは手首座標を正しく算出できないため，結果としてジェスチャの条件を満たさず，認識失敗に繋がったと考えられる．

第二の要因として，カメラアングルおよびオクルージョン（遮蔽）の影響が挙げられる．
本実験で使用した映像は一般的な中継アングルであり，審判が常にカメラに対して正対しているわけではない．審判がカメラに背を向けている場合や，選手と重なっている場合，あるいは画面の端で見切れている場合においては，骨格推定モデルによるキーポイント抽出の信頼度が著しく低下する傾向が見られた．特に，腕が体幹と重なって見えるアングルでは，手首の位置を正確に特定できず，誤った座標が出力されることで判別ミスを誘発した可能性が高い．

以上のことから，ジェスチャ認識の精度向上には，単にモデルの認識能力を高めるだけでなく，腕を挙げた状態を十分に含む多様な学習データの拡充や，バウンディングボックスのサイズ調整といった処理の改善が不可欠であると結論付けられる．
\chapter{まとめ}

本研究では，バスケットボールの試合映像を用いたスタッツ自動収集の実現に向け，従来の選手位置情報のみに依存した判別手法が抱える課題を解決するために，審判のジェスチャ認識を統合した新たなシュート種類判別システムを提案した．

従来のホモグラフィ変換を用いた位置推定手法は，原理的な信頼性は高いものの，オクルージョンや検出漏れによって未検出となるケースが存在した．これに対し，本研究ではバスケットボールの競技規則に着目し，審判が提示するハンドジェスチャ骨格認識モデルを用いて認識することで，シュート種類の判別を補完するアプローチを試みた．

Bリーグの試合映像を用いた評価実験の結果，以下の知見が得られた．
第一に，位置推定手法単体では，オクルージョン等により2ポイントシュートの約10.8\%が未検出となった一方で，提案する統合手法を用いることで，これらを効果的に補完できることが確認された．
第二に，統合手法による判別結果は，2ポイントシュートで96.95\%，3ポイントシュートで95.12\%の検出率を達成した．これは位置推定単体と比較して，それぞれ約7.7ポイント，4.0ポイントの向上であり，提案手法の有効性が定量的に示されたといえる．
以上の結果より，審判のジェスチャ情報は，映像解析によるシュート判別において極めて有用な特徴量であり，位置情報と統合することでシステムの検出率を大幅に向上させることが可能であると結論付けられる．

本研究では統合手法の有効性を示したが，実用的な自動スタッツ収集システムの構築に向けては，いくつかの課題が残されている．今後の課題として，以下の3点が挙げられる．

第一に，シュート動画の自動抽出である．
本実験では，元動画より手動でシュートシーンを切り出しているが，システムの実用化には，試合全体の映像からシュートが行われたフレーム範囲を特定し，自動的に動画を生成する機能が不可欠である．これには，ボールの軌道認識や選手の行動認識技術を応用し，一連のシュート動作の開始と終了を自動的に判別するアルゴリズムの実装が必要である．

第二に，ホモグラフィ変換行列の算出に用いる特徴点の自動検出である．
現状では，ホモグラフィ変換のための対応点を手動で設定している．しかし，カメラアングルが頻繁に変化する実際の放送映像へ適用するためには，この過程の自動化が求められる．コート内のペイントエリア領域やラインをセグメンテーション技術等を用いて認識し，フレームごとにホモグラフィ変換行列を更新する手法の検討が必要である．

第三に，2ポイントシュート判別における誤検出への対応である．
第4章の考察で述べた通り，3ポイントシュートのジェスチャは単純な動作であるため，ファウルやタイムアウトなどの他のシグナルと誤認しやすいという課題がある．この解決に対しては，静的な姿勢情報だけでなく，動作の時系列変化を考慮した認識手法の導入を検討する．具体的には，Wangらの研究\cite{wang}で提案されている時空間特徴を用いた手法等を参考にし，紛らわしい動作と正規のジェスチャを高精度に分離するモデルの構築を目指す．

% \appendix
% \chapter{Latexの書き方}

\acknowledgement %謝辞

北村泰彦教授には、私が他研究室の所属であることを顧みず、快く研究室の一員として受け入れていただきました。ゼミへの参加をご快諾いただき、所属学生と分け隔てなく熱心にご指導いただいたことは、本研究の遂行において不可欠でした。先生の寛大なご配慮と温かいご指導に、心より御礼申し上げます。

山本倫也教授には，学期途中での研究室変更という事態に際しましても，多大なるご配慮と柔軟なご対応をいただきました．また，それに伴う研究テーマの変更につきましても，有益なご助言と温かいご指導を賜りましたこと，この場を借りて深く感謝申し上げます．

北村研究室の同輩諸氏には，
日頃から多くのご助言やご支援をいただき，大変感謝しています．

また，山本研究室の同輩諸氏には，
特に審査前において多くのご助言やご支援をいただき，大変感謝しています．

最後に，4年間大学に通わせてくれた祖父母に，心から感謝しています．

\bibliographystyle{junsrt}
\bibliography{sample}

\end{document}